#!/usr/bin/env bash

###############################################################################
# ppdb-prod environment setup script
###############################################################################

# Prevent execution â€” this script must be sourced
if [[ "${BASH_SOURCE[0]}" == "${0}" ]]; then
  echo "ERROR: This script must be sourced, not executed." >&2
  exit 1
fi

# Include the base environment setup
. "$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"/_base.env

# Set the variable for the production environment.
export PPDB_ENV="prod"

# == GCP ENVIRONMENT ==

# Set the GCP project name.
export GCP_PROJECT="ppdb-prod"

# Set the service account name.
export SERVICE_ACCOUNT_NAME="ppdb-storage-manager"

# Set the account credentials file.
export GOOGLE_APPLICATION_CREDENTIALS="$HOME/.gcp/${GCP_PROJECT}/keys/${SERVICE_ACCOUNT_NAME}-key.json"
if [ ! -f "$GOOGLE_APPLICATION_CREDENTIALS" ]; then
  echo "ERROR: Google Application Credentials file does not exist: $GOOGLE_APPLICATION_CREDENTIALS"
  return 1
fi

# Set the current service account.
export SERVICE_ACCOUNT_EMAIL="${SERVICE_ACCOUNT_NAME}@${GCP_PROJECT}.iam.gserviceaccount.com"
gcloud auth activate-service-account --key-file="$GOOGLE_APPLICATION_CREDENTIALS"
gcloud config set account ${SERVICE_ACCOUNT_EMAIL}

# Set the current project.
gcloud config set project ${GCP_PROJECT}

# This is the primary GCS bucket used for PPDB file staging.
export GCS_BUCKET="rubin-ppdb-lsstcam"

# Default GCP region.
# FIXME: Should this be set using `gcloud config set`?
export GCP_REGION="us-central1"

# The dataset ID for the PPDB in BigQuery.
export DATASET_ID="ppdb_lsstcam"

# == AWS ENVIRONMENT ==

# This is needed mainly for reading APDB config files from S3.
export AWS_SHARED_CREDENTIALS_FILE=$HOME/.aws/aws-credentials.ini

# == PPDB REPLICATION ENVIRONMENT ==

# These correspond to the Phalanx environment variables for the ppdb-replication app.
export APDB_CONFIG_FILE="s3://embargo@rubin-summit-users/apdb_config/cassandra/pp_apdb_lsstcam.yaml"
export PPDB_SCHEMA_NAME="ppdb_dm51379"
export PPDB_CONFIG_FILE="/sdf/data/rubin/user/jeremym/dev/DM-50563/ppdb-config/${PPDB_SCHEMA_NAME}.yaml"
if [ ! -f "$PPDB_CONFIG_FILE" ]; then
  echo "ERROR: PPDB config file does not exist: $PPDB_CONFIG_FILE"
  return 1
fi
export PPDB_STAGING_DIR="/sdf/scratch/rubin/ppdb/staging/lsstcam"
export PPDB_DB_URL="postgresql://rubin@usdf-prompt-processing-dev.slac.stanford.edu:5432/lsst-devl"
export PPDB_CONFIG_DIR="/sdf/data/rubin/user/jeremym/dev/DM-50563/ppdb-config"

export LOG_LEVEL="INFO"

# FIXME: dax_ppdb and dax_apdb should really have sdm_schemas as a proper Python dependency.
export SDM_SCHEMAS_DIR="$HOME/.ppdb/sdm_schemas"
if [ ! -d "$SDM_SCHEMAS_DIR" ]; then
  echo "ERROR: SDM schemas directory does not exist: $SDM_SCHEMAS_DIR"
  return 1
fi

# GKE cluster setup
# TODO: This could be its own generic setup script like `_gke.env` or `_post.env` for all post-env setups.
export GKE_WORKLOAD_POOL="${GCP_PROJECT}.svc.id.goog"
